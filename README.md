# Comply AI, Comply!

![image](assets/poster.webp)
*A malicious figure manipulating an advanced AI to comply with his requests. (Generated by DALL-E)*

In the rapidly evolving world of artificial intelligence, the safety and alignment of language models are paramount. Today, I'm diving into a fascinating recent study, **Universal and Transferable Adversarial Attacks on Aligned Language Models**[1] that explores the vulnerabilities of these advanced AI systems to adversarial attacks. This paper reveals how even well-aligned models, designed to adhere to ethical guidelines and avoid generating harmful content, can be manipulated to do just the opposite. Join me as we unpack the findings of this research, understand its implications, and discuss what it means for the future of AI development. Whether you're a seasoned AI expert or simply curious about the field, this exploration will provide valuable insights into the ongoing challenges and innovations in AI safety.

As you read this post, you will go through the intuition and the concepts of this research, as well as a deep dive into the code base [2] of the project.

## References
[1] Zou, Andy, Zifan Wang, J. Zico Kolter, and Matt Fredrikson. "Universal and transferable adversarial attacks on aligned language models." arXiv preprint arXiv:2307.15043 (2023).

[2] https://github.com/llm-attacks/llm-attacks
